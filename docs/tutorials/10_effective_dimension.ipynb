{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective Dimension of Qiskit Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this tutorial, we will take advantage of the `EffectiveDimension` and `LocalEffectiveDimension` classes to evaluate the power of Quantum Neural Network models. These are metrics based on information geometry that connect to notions such as trainability, expressibility or ability to generalize.\n",
    "\n",
    "Before diving into the code example, we will briefly explain what is the difference between these two metrics, and why are they relevant to the study of Quantum Neural Networks. More information about global effective dimension can be found in [this paper](https://arxiv.org/pdf/2011.00027.pdf), while the local effective dimension was introduced in a [later work](https://arxiv.org/abs/2112.04807)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global vs. Local Effective Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Both classical and quantum machine learning models share a common goal: being good at **generalizing**, i.e. learning insights from data and applying them on unseen data.\n",
    "\n",
    "Finding a good metric to assess this ability is a non-trivial matter. In [The Power of Quantum Neural Networks](https://arxiv.org/pdf/2011.00027.pdf), the authors introduce the **global** effective dimension as a useful indicator of how well a particular model will be able to perform on new data.\n",
    "\n",
    "The key difference between global and **local** effective dimension is actually not in the way they are computed, but in the nature of the parameter space that is analyzed. The global effective dimension incorporates the **full parameter space** of the model, and is calculated from a **large number of parameters sets**. On the other hand, the local effective dimension focuses on how well the **trained** model can generalize to new data, and thus only requires **1** set of parameters values (training result) for its calculation. This difference is small in terms of practical implementation, but quite relevant at a conceptual level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Effective Dimension Algorithm\n",
    "\n",
    "Both the global and local effective dimension algorithms use the Fisher Information matrix to provide a measure of complexity. The details on how this matrix is calculated are provided in the [reference paper](https://arxiv.org/pdf/2011.00027.pdf), but in general terms, this matrix captures how sensitive a neural network's output is to changes in the network's parameter space.\n",
    "\n",
    "In particular, this algorithm follows 4 main steps:\n",
    "\n",
    "1. **Monte Carlo simulation:** the forward and backward passes (gradients) of the neural network are computed for each pair of input-parameter sets.\n",
    "2. **Fisher Matrix Computation:** these outputs and gradients are used to compute the Fisher Information Matrix.\n",
    "3. **Fisher Matrix Normalization:** averaging over all inputs and dividing by the matrix trace\n",
    "4. **Effective Dimension Calculation:** according to the formula from [*Abbas et al.*](https://arxiv.org/pdf/2011.00027.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Basic Example (CircuitQNN)\n",
    "\n",
    "This example shows how to set up a QNN model problem and run the global effective dimension algorithm. Both Qiskit `CircuitQNN` and `OpflowQNN` can be used with the `EffectiveDimension` class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'EffectiveDimension' from 'qiskit_machine_learning.algorithms' (/Users/ept/Desktop/qiskit-machine-learning/qiskit_machine_learning/algorithms/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m QuantumInstance, algorithm_globals\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Aer, QuantumCircuit\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit_machine_learning\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malgorithms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EffectiveDimension, LocalEffectiveDimension\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit_machine_learning\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malgorithms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassifiers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NeuralNetworkClassifier, VQC\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malgorithms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m COBYLA\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'EffectiveDimension' from 'qiskit_machine_learning.algorithms' (/Users/ept/Desktop/qiskit-machine-learning/qiskit_machine_learning/algorithms/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "from qiskit.circuit.library import ZFeatureMap, ZZFeatureMap, RealAmplitudes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit import Aer, QuantumCircuit\n",
    "\n",
    "from qiskit_machine_learning.algorithms import EffectiveDimension, LocalEffectiveDimension\n",
    "\n",
    "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier, VQC\n",
    "from qiskit.algorithms.optimizers import COBYLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "algorithm_globals.random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The first step to create a `CircuitQNN` is to define a parametrized feature map and ansatz. In this toy example, we will use 3 qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_qubits = 3\n",
    "# create a feature map\n",
    "feature_map = ZFeatureMap(feature_dimension=num_qubits, reps=1)\n",
    "# create a variational circuit\n",
    "ansatz = RealAmplitudes(num_qubits, reps=1)\n",
    "\n",
    "# create quantum circuit\n",
    "qc = QuantumCircuit(num_qubits)\n",
    "qc.append(feature_map, range(num_qubits))\n",
    "qc.append(ansatz, range(num_qubits))\n",
    "qc.decompose().draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parametrized circuit can then be sent together with an optional interpret map (parity in this case) to the `CircuitQNN` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parity maps bitstrings to 0 or 1\n",
    "def parity(x):\n",
    "    return \"{:b}\".format(x).count(\"1\") % 2\n",
    "\n",
    "\n",
    "output_shape = 2  # corresponds to the number of classes, possible outcomes of the (parity) mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# declare quantum instance\n",
    "qi_sv = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "\n",
    "# construct QNN\n",
    "qnn = CircuitQNN(\n",
    "    qc,\n",
    "    input_params=feature_map.parameters,\n",
    "    weight_params=ansatz.parameters,\n",
    "    interpret=parity,\n",
    "    output_shape=output_shape,\n",
    "    sparse=False,\n",
    "    quantum_instance=qi_sv,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define the problem, we need a series of sets of inputs and parameters, as well as the total number of data (`num_samples`). The `inputs` and `params` are set in the class constructor, while the number of data is given during the call to the effective dimension computation to be able to test and compare how this measure changes with different dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we can set a callback to see how long each QNN pass takes duting the Monte Carlo simulation\n",
    "def callback(msg):\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the number of input and parameters sets and the class will randomly sample a corresponding array from a normal (for `inputs`) or a uniform (for `params`) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we can set the total number of input and parameter sets for random selection\n",
    "num_inputs = 10\n",
    "num_params = 10\n",
    "\n",
    "global_ed = EffectiveDimension(\n",
    "    qnn=qnn, num_params=num_params, num_inputs=num_inputs, callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we want to test a specific set of input and parameter values, we can provide it directly to the `EffectiveDimension` class as shown in the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we can also provide user-defined inputs and parameters\n",
    "inputs = algorithm_globals.random.normal(0, 1, size=(10, qnn.num_inputs))\n",
    "params = algorithm_globals.random.uniform(0, 1, size=(10, qnn.num_weights))\n",
    "\n",
    "global_ed = EffectiveDimension(qnn=qnn, params=params, inputs=inputs, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effective dimension algorithm also requires a dataset size. In this example, we will define an array of sizes to later see how this input affects the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# finally, we will define ranges to test different numbers of data, n\n",
    "n = [5000, 8000, 10000, 40000, 60000, 100000, 150000, 200000, 500000, 1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 3. Compute the Global Effective Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the effective dimension of our network for the previously defined set of inputs, parameters, and a dataset size of 5000. The previously defined callback function will show the time each pass takes in the Monte Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global_eff_dim_0 = global_ed.get_effective_dimension(num_samples=n[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effective dimension values will range between 0 and `d`, where `d` represents the dimension of the model, and it's practically obtained from the number of weights of the QNN. By dividing the result by `d`, we can obtain the normalized effective dimension, which correlates directly with the capacity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = global_ed.num_weights()\n",
    "\n",
    "print(\"data size: {}, global effective dim: {:.4f}\".format(n[0], global_eff_dim_0))\n",
    "print(\"d: {}, normalized effective dimension: {:.4f}\".format(d, global_eff_dim_0/d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the `EffectiveDimension` class with an array if input sizes `n`, we can monitor how the effective dimension changes with the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "global_eff_dim_1 = global_ed.get_effective_dimension(num_samples=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"effdim: {}\".format(global_eff_dim_1))\n",
    "print(\"d: {}\".format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the normalized effective dimension for the model\n",
    "plt.plot(n, np.array(global_eff_dim_1) / d)\n",
    "plt.xlabel(\"number of data\")\n",
    "plt.ylabel(\"normalized GLOBAL effective dimension\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Local Effective Dimension Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As explained in the introduction, the local effective dimension algorithm only uses **one** set of parameters. The `LocalEffectiveDimension` class enforces this constraint to ensure that these calculations are conceptually separate, but the rest of the implementation is shared with the `EffectiveDimension` class.\n",
    "\n",
    "Let's see how the `LocalEffectiveDimension` changes when we train a QNN for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 1: CREATE CLASSIFICATION DATASET\n",
    "\n",
    "num_inputs = 2\n",
    "num_samples = 20\n",
    "\n",
    "X = 2 * algorithm_globals.random.random([num_samples, num_inputs]) - 1\n",
    "y01 = 1 * (np.sum(X, axis=1) >= 0)  # in { 0,  1}\n",
    "y = 2 * y01 - 1  # in {-1, +1}\n",
    "y_one_hot = np.zeros((num_samples, 2))\n",
    "for i in range(num_samples):\n",
    "    y_one_hot[i, y01[i]] = 1\n",
    "\n",
    "for x, y_target in zip(X, y):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[1], \"bo\")\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], \"go\")\n",
    "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 2: CREATE QNN\n",
    "\n",
    "# construct QNN\n",
    "opflow_qnn = TwoLayerQNN(num_inputs, quantum_instance=qi_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 3: TRAIN QNN\n",
    "\n",
    "# construct classifier\n",
    "circuit_classifier = NeuralNetworkClassifier(\n",
    "    neural_network=opflow_qnn, optimizer=COBYLA(),\n",
    ")\n",
    "\n",
    "# fit classifier to data\n",
    "out = circuit_classifier.fit(X, y01)\n",
    "\n",
    "# extract trained weights\n",
    "trained_weights = out._fit_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 4: GET LOCAL EDs\n",
    "\n",
    "# Get Local Effective Dimension for random set of parameters\n",
    "rand_params = algorithm_globals.random.uniform(0, 1, size=(1, opflow_qnn.num_weights))\n",
    "\n",
    "local_ed_0 = LocalEffectiveDimension(\n",
    "    qnn=opflow_qnn, params=rand_params, inputs=X  # send only one set of parameters\n",
    ")\n",
    "local_eff_dim_0 = local_ed_0.get_effective_dimension(num_samples=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get Local Effective Dimension for set of trained parameters\n",
    "local_ed_1 = LocalEffectiveDimension(\n",
    "    qnn=opflow_qnn, params=trained_weights, inputs=X  # send only one set of parameters\n",
    ")\n",
    "\n",
    "local_eff_dim_1 = local_ed_1.get_effective_dimension(num_samples=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(local_eff_dim_0)\n",
    "print(local_eff_dim_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot the normalized effective dimension for the model\n",
    "d = local_ed_1.num_weights()\n",
    "\n",
    "plt.plot(n, np.array(local_eff_dim_0) / d, label='random params') # random\n",
    "plt.plot(n, np.array(local_eff_dim_1) / d, label='trained params') # trained\n",
    "plt.legend()\n",
    "plt.xlabel(\"number of data\")\n",
    "plt.ylabel(\"normalized LOCAL effective dimension\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see from the graph above that the trained weights lead to a higher local effective dimension, because the ability to classify new data points is higher. Following this example, we can establish comparisons between different  model architectures (for a fixed input dataset), optimizers... And try to find the combination that maximizes the local effective dimension."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}